{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.16.2)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sklearn) (0.21.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 140\r\n",
      "drwxr-xr-x.  3 jovyan users    18 Jul 18 17:27 '~'\r\n",
      "drwxr-xr-x.  6 jovyan users  4096 Jul 18 17:57  .\r\n",
      "drwxr-xr-x. 11 jovyan users  4096 Jul 18 17:15  ..\r\n",
      "-rw-r--r--.  1 jovyan users     0 Jul 18 17:57  body_pp.dpkl\r\n",
      "-rw-r--r--.  1 jovyan users   223 Jul 18 17:15  Dockerfile\r\n",
      "-rw-r--r--.  1 jovyan users   748 Jul 18 17:15  Dockerfile.estimator\r\n",
      "-rw-r--r--.  1 jovyan users    77 Jul 18 17:15  environment_seldon_rest\r\n",
      "lrwxrwxrwx.  1 jovyan users     7 Jul 18 17:48  github-issues-data -> jd_data\r\n",
      "drwxr-xr-x.  2 jovyan users    39 Jul 18 17:19  .ipynb_checkpoints\r\n",
      "-rw-r--r--.  1 jovyan users  1243 Jul 18 17:15  IssueSummarization.py\r\n",
      "drwxr-xr-x.  2 jovyan users    56 Jul 18 17:47  jd_data\r\n",
      "-rw-r--r--.  1 jovyan users  4285 Jul 18 17:15  Makefile\r\n",
      "-rw-r--r--.  1 jovyan users    74 Jul 18 17:15  requirements.txt\r\n",
      "-rw-r--r--.  1 jovyan users 14833 Jul 18 17:15  seq2seq_utils.py\r\n",
      "drwxr-xr-x.  3 jovyan users    32 Jul 18 17:42  test_data\r\n",
      "-rw-r--r--.  1 jovyan users 10766 Jul 18 17:15  trainer.py\r\n",
      "-rw-r--r--.  1 jovyan users 61918 Jul 18 17:57  Training.ipynb\r\n",
      "-rw-r--r--.  1 jovyan users  6548 Jul 18 17:15  train.py\r\n",
      "-rw-r--r--.  1 jovyan users  2546 Jul 18 17:15  train_test.py\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the github-issues-data volume is mounted in /mnt\n",
    "!ls -la ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_DIR=jd_data\n",
      "jovyan\n",
      "/home/jovyan/examples/github_issue_summarization/notebooks\n",
      "total 132\n",
      "-rw-r--r--. 1 jovyan users   748 Jul 18 17:15  Dockerfile.estimator\n",
      "-rw-r--r--. 1 jovyan users   223 Jul 18 17:15  Dockerfile\n",
      "-rw-r--r--. 1 jovyan users 14833 Jul 18 17:15  seq2seq_utils.py\n",
      "-rw-r--r--. 1 jovyan users    74 Jul 18 17:15  requirements.txt\n",
      "-rw-r--r--. 1 jovyan users  4285 Jul 18 17:15  Makefile\n",
      "-rw-r--r--. 1 jovyan users  1243 Jul 18 17:15  IssueSummarization.py\n",
      "-rw-r--r--. 1 jovyan users    77 Jul 18 17:15  environment_seldon_rest\n",
      "-rw-r--r--. 1 jovyan users  2546 Jul 18 17:15  train_test.py\n",
      "-rw-r--r--. 1 jovyan users  6548 Jul 18 17:15  train.py\n",
      "-rw-r--r--. 1 jovyan users 10766 Jul 18 17:15  trainer.py\n",
      "drwxr-xr-x. 3 jovyan users    18 Jul 18 17:27 '~'\n",
      "drwxr-xr-x. 3 jovyan users    32 Jul 18 17:42  test_data\n",
      "lrwxrwxrwx. 1 jovyan users     7 Jul 18 17:48  github-issues-data -> jd_data\n",
      "-rw-r--r--. 1 jovyan users 61918 Jul 18 17:57  Training.ipynb\n",
      "-rw-r--r--. 1 jovyan users     0 Jul 18 17:57  body_pp.dpkl\n",
      "drwxr-xr-x. 2 jovyan users     6 Jul 18 17:58  jd_data\n",
      "total 132\n",
      "-rw-r--r--. 1 jovyan users   748 Jul 18 17:15  Dockerfile.estimator\n",
      "-rw-r--r--. 1 jovyan users   223 Jul 18 17:15  Dockerfile\n",
      "-rw-r--r--. 1 jovyan users 14833 Jul 18 17:15  seq2seq_utils.py\n",
      "-rw-r--r--. 1 jovyan users    74 Jul 18 17:15  requirements.txt\n",
      "-rw-r--r--. 1 jovyan users  4285 Jul 18 17:15  Makefile\n",
      "-rw-r--r--. 1 jovyan users  1243 Jul 18 17:15  IssueSummarization.py\n",
      "-rw-r--r--. 1 jovyan users    77 Jul 18 17:15  environment_seldon_rest\n",
      "-rw-r--r--. 1 jovyan users  2546 Jul 18 17:15  train_test.py\n",
      "-rw-r--r--. 1 jovyan users  6548 Jul 18 17:15  train.py\n",
      "-rw-r--r--. 1 jovyan users 10766 Jul 18 17:15  trainer.py\n",
      "drwxr-xr-x. 3 jovyan users    18 Jul 18 17:27 '~'\n",
      "drwxr-xr-x. 3 jovyan users    32 Jul 18 17:42  test_data\n",
      "lrwxrwxrwx. 1 jovyan users     7 Jul 18 17:48  github-issues-data -> jd_data\n",
      "-rw-r--r--. 1 jovyan users 61918 Jul 18 17:57  Training.ipynb\n",
      "-rw-r--r--. 1 jovyan users     0 Jul 18 17:57  body_pp.dpkl\n",
      "drwxr-xr-x. 2 jovyan users     6 Jul 18 17:58  jd_data\n",
      "/home/jovyan/examples/github_issue_summarization/notebooks\n",
      "total 0\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Set path for data dir\n",
    "%env DATA_DIR=jd_data\n",
    "!whoami\n",
    "!pwd\n",
    "!rm -rf ${DATA_DIR}\n",
    "!mkdir ${DATA_DIR}\n",
    "!ls -ltr\n",
    "!cd ${DATA_DIR}\n",
    "!ls -ltr\n",
    "!pwd\n",
    "!ls -ltr ${DATA_DIR}\n",
    "!echo DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-18 17:59:04--  https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.210.240, 2a00:1450:4009:817::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.210.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1027424178 (980M) [application/zip]\n",
      "Saving to: ‘jd_data/github-issues.zip’\n",
      "\n",
      "github-issues.zip   100%[===================>] 979.83M  24.7MB/s    in 41s     \n",
      "\n",
      "2019-07-18 17:59:45 (24.2 MB/s) - ‘jd_data/github-issues.zip’ saved [1027424178/1027424178]\n",
      "\n",
      "total 92\n",
      "-rw-r--r--. 1 jovyan users   748 Jul 18 17:15  Dockerfile.estimator\n",
      "-rw-r--r--. 1 jovyan users   223 Jul 18 17:15  Dockerfile\n",
      "-rw-r--r--. 1 jovyan users 14833 Jul 18 17:15  seq2seq_utils.py\n",
      "-rw-r--r--. 1 jovyan users    74 Jul 18 17:15  requirements.txt\n",
      "-rw-r--r--. 1 jovyan users  4285 Jul 18 17:15  Makefile\n",
      "-rw-r--r--. 1 jovyan users  1243 Jul 18 17:15  IssueSummarization.py\n",
      "-rw-r--r--. 1 jovyan users    77 Jul 18 17:15  environment_seldon_rest\n",
      "-rw-r--r--. 1 jovyan users  2546 Jul 18 17:15  train_test.py\n",
      "-rw-r--r--. 1 jovyan users  6548 Jul 18 17:15  train.py\n",
      "-rw-r--r--. 1 jovyan users 10766 Jul 18 17:15  trainer.py\n",
      "drwxr-xr-x. 3 jovyan users    18 Jul 18 17:27 '~'\n",
      "drwxr-xr-x. 3 jovyan users    32 Jul 18 17:42  test_data\n",
      "lrwxrwxrwx. 1 jovyan users     7 Jul 18 17:48  github-issues-data -> jd_data\n",
      "-rw-r--r--. 1 jovyan users     0 Jul 18 17:57  body_pp.dpkl\n",
      "-rw-r--r--. 1 jovyan users 21891 Jul 18 17:59  Training.ipynb\n",
      "drwxr-xr-x. 2 jovyan users    31 Jul 18 17:59  jd_data\n",
      "Archive:  jd_data/github-issues.zip\n",
      "  inflating: jd_data/github_issues.csv  \n"
     ]
    }
   ],
   "source": [
    "# Download the github-issues.zip training data to /mnt/github-issues-data\n",
    "!mkdir -p ${DATA_DIR}\n",
    "!wget --directory-prefix=${DATA_DIR} https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip\n",
    "!ls -ltr\n",
    "    \n",
    "# Unzip the file into /mnt/github-issues-data directory\n",
    "!unzip ${DATA_DIR}/github-issues.zip -d ${DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 92\r\n",
      "-rw-r--r--. 1 jovyan users   748 Jul 18 17:15  Dockerfile.estimator\r\n",
      "-rw-r--r--. 1 jovyan users   223 Jul 18 17:15  Dockerfile\r\n",
      "-rw-r--r--. 1 jovyan users 14833 Jul 18 17:15  seq2seq_utils.py\r\n",
      "-rw-r--r--. 1 jovyan users    74 Jul 18 17:15  requirements.txt\r\n",
      "-rw-r--r--. 1 jovyan users  4285 Jul 18 17:15  Makefile\r\n",
      "-rw-r--r--. 1 jovyan users  1243 Jul 18 17:15  IssueSummarization.py\r\n",
      "-rw-r--r--. 1 jovyan users    77 Jul 18 17:15  environment_seldon_rest\r\n",
      "-rw-r--r--. 1 jovyan users  2546 Jul 18 17:15  train_test.py\r\n",
      "-rw-r--r--. 1 jovyan users  6548 Jul 18 17:15  train.py\r\n",
      "-rw-r--r--. 1 jovyan users 10766 Jul 18 17:15  trainer.py\r\n",
      "drwxr-xr-x. 3 jovyan users    18 Jul 18 17:27 '~'\r\n",
      "drwxr-xr-x. 3 jovyan users    32 Jul 18 17:42  test_data\r\n",
      "-rw-r--r--. 1 jovyan users     0 Jul 18 17:57  body_pp.dpkl\r\n",
      "-rw-r--r--. 1 jovyan users 21891 Jul 18 17:59  Training.ipynb\r\n",
      "drwxr-xr-x. 2 jovyan users    56 Jul 18 17:59  jd_data\r\n",
      "lrwxrwxrwx. 1 jovyan users     7 Jul 18 18:00  github-issues-data -> jd_data\r\n"
     ]
    }
   ],
   "source": [
    "# Create a symlink from <current_directory>/github-issues-data to /mnt/github-issues-data\n",
    "!rm -rf github-issues-data\n",
    "!ln -sf ${DATA_DIR} github-issues-data\n",
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 jovyan users 2.7G Jan 17  2018 github-issues-data/github_issues.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the github-issues-data symlink is created\n",
    "!ls -lh github-issues-data/github_issues.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set and preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,800 rows 3 columns\n",
      "Test: 200 rows 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4808004</th>\n",
       "      <td>\"https://github.com/turretcss/turretcss/issues/14\"</td>\n",
       "      <td>typo it's &gt; its</td>\n",
       "      <td>makes element cover it's container using .cover; mixin it's &gt; its</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151120</th>\n",
       "      <td>\"https://github.com/Brewskey/brewskey.js-api/issues/25\"</td>\n",
       "      <td>flow lib export is broken</td>\n",
       "      <td>when we use the lib in other project. import types { sometipe } works correct. but daoapi.createfilter -or whatever from daoapi is not typechecked. need to figure out correct way to export default in flow module declarations. there are some work around here: https://github.com/facebook/flow/issues/1806 i was trying from that: declare type daoapi = { createfilter: ... ..., } declare export default daoapi; but seems this doesn't work :/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4904563</th>\n",
       "      <td>\"https://github.com/bluekyu/panda3d/issues/1\"</td>\n",
       "      <td>add additional thirdparties</td>\n",
       "      <td>- directcam - vision - tiff - openexr see also https://github.com/bluekyu/panda3d-thirdparty/issues/2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       issue_url  \\\n",
       "4808004       \"https://github.com/turretcss/turretcss/issues/14\"   \n",
       "1151120  \"https://github.com/Brewskey/brewskey.js-api/issues/25\"   \n",
       "4904563            \"https://github.com/bluekyu/panda3d/issues/1\"   \n",
       "\n",
       "                         issue_title  \\\n",
       "4808004              typo it's > its   \n",
       "1151120    flow lib export is broken   \n",
       "4904563  add additional thirdparties   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                           body  \n",
       "4808004                                                                                                                                                                                                                                                                                                                                                                                       makes element cover it's container using .cover; mixin it's > its  \n",
       "1151120  when we use the lib in other project. import types { sometipe } works correct. but daoapi.createfilter -or whatever from daoapi is not typechecked. need to figure out correct way to export default in flow module declarations. there are some work around here: https://github.com/facebook/flow/issues/1806 i was trying from that: declare type daoapi = { createfilter: ... ..., } declare export default daoapi; but seems this doesn't work :/  \n",
       "4904563                                                                                                                                                                                                                                                                                                                                                   - directcam - vision - tiff - openexr see also https://github.com/bluekyu/panda3d-thirdparty/issues/2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file='github-issues-data/github_issues.csv'\n",
    "\n",
    "\n",
    "# read in data sample 2000 rows (for speed of tutorial)\n",
    "# Set this to False to train on the entire dataset\n",
    "use_sample_data=True\n",
    "\n",
    "if use_sample_data:\n",
    "    training_data_size=2000\n",
    "    traindf, testdf = train_test_split(pd.read_csv(data_file).sample(n=training_data_size), \n",
    "                                   test_size=.10)\n",
    "else:\n",
    "    traindf, testdf = train_test_split(pd.read_csv(data_file),test_size=.10)\n",
    "\n",
    "\n",
    "#print out stats about shape of data\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "\n",
    "# preview data\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to lists in preparation for modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"makes element cover it's container using .cover; mixin it's > its\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_body_raw = traindf.body.tolist()\n",
    "train_title_raw = traindf.issue_title.tolist()\n",
    "#preview output of first element\n",
    "train_body_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data For Deep Learning\n",
    "\n",
    "See [this repo](https://github.com/hamelsmu/ktext) for documentation on the ktext package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ktext in /opt/conda/lib/python3.6/site-packages (0.40)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from ktext) (1.16.2)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.6/site-packages (from ktext) (7.1.0)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.6/site-packages (from ktext) (1.13.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from ktext) (5.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from ktext) (1.2.1)\n",
      "Requirement already satisfied: msgpack in /opt/conda/lib/python3.6/site-packages (from ktext) (0.6.1)\n",
      "Requirement already satisfied: dask in /opt/conda/lib/python3.6/site-packages (from ktext) (2.1.0)\n",
      "Requirement already satisfied: pandas>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from ktext) (0.24.2)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from ktext) (0.2.4)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.6/site-packages (from ktext) (0.14.0)\n",
      "Requirement already satisfied: keras==2.2.4 in /opt/conda/lib/python3.6/site-packages (from ktext) (2.2.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from ktext) (1.12.0)\n",
      "Requirement already satisfied: msgpack-numpy in /opt/conda/lib/python3.6/site-packages (from ktext) (0.4.4.3)\n",
      "Requirement already satisfied: textacy==0.6.2 in /opt/conda/lib/python3.6/site-packages (from ktext) (0.6.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (0.7.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (3.7.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (1.0.9)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (0.33.1)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (1.13.1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (1.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (1.19.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (1.0.7)\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow->ktext) (0.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.21.0->ktext) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas>=0.21.0->ktext) (2018.9)\n",
      "Requirement already satisfied: dill>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from pathos->ktext) (0.3.0)\n",
      "Requirement already satisfied: pox>=0.2.6 in /opt/conda/lib/python3.6/site-packages (from pathos->ktext) (0.2.6)\n",
      "Requirement already satisfied: ppft>=1.6.6.1 in /opt/conda/lib/python3.6/site-packages (from pathos->ktext) (1.6.6.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.8 in /opt/conda/lib/python3.6/site-packages (from pathos->ktext) (0.70.8)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras==2.2.4->ktext) (2.9.0)\n",
      "Requirement already satisfied: pyphen>=0.9.4 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (0.9.5)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (0.10.0)\n",
      "Requirement already satisfied: python-levenshtein>=0.12.0 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (0.12.0)\n",
      "Requirement already satisfied: unidecode>=0.04.19 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (1.1.1)\n",
      "Requirement already satisfied: spacy>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (2.1.6)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.2.0 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (4.4.3)\n",
      "Requirement already satisfied: scikit-learn>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (0.21.2)\n",
      "Requirement already satisfied: networkx>=1.11 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (2.3)\n",
      "Requirement already satisfied: tqdm>=4.11.1 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (4.32.2)\n",
      "Requirement already satisfied: ijson>=2.3 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (2.4)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (3.1.0)\n",
      "Requirement already satisfied: pyemd>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (0.5.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /opt/conda/lib/python3.6/site-packages (from textacy==0.6.2->ktext) (2.21.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow->ktext) (40.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->ktext) (3.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->ktext) (0.15.2)\n",
      "Requirement already satisfied: mock>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->ktext) (2.0.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from cytoolz>=0.8.0->textacy==0.6.2->ktext) (0.10.0)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (7.0.8)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (2.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (0.2.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (1.0.2)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (0.2.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (0.0.7)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.0.0->textacy==0.6.2->ktext) (2.0.2)\n",
      "Requirement already satisfied: html5lib in /opt/conda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.2.0->textacy==0.6.2->ktext) (1.0.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.2.0->textacy==0.6.2->ktext) (0.1.7)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.17.0->textacy==0.6.2->ktext) (0.13.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=1.11->textacy==0.6.2->ktext) (4.4.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.10.0->textacy==0.6.2->ktext) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.10.0->textacy==0.6.2->ktext) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.10.0->textacy==0.6.2->ktext) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.10.0->textacy==0.6.2->ktext) (1.24.1)\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.6/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->ktext) (5.1.3)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy==0.6.2->ktext) (0.5.1)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install ktext\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ktext.preprocess import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 0 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 1,800 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 0 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " makes element cover it's container using .cover; mixin it's > its \n",
      "\n",
      "after pre-processing:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0  868  629 2739   10   30  387   45 2739 2342   10   30  304] \n",
      "\n",
      "CPU times: user 156 ms, sys: 333 ms, total: 489 ms\n",
      "Wall time: 914 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
    "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
    "#  to 1 which will become common index for rare words \n",
    "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
    "train_body_vecs = body_pp.fit_transform(train_body_raw)\n",
    "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
    "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at one example of processed issue bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " makes element cover it's container using .cover; mixin it's > its \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_body_vecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-742efeab5392>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\noriginal string:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_body_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after pre-processing:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_body_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_body_vecs' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
    "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 0 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 1,800 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 0 sec\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a text processor for the titles, with some different parameters\n",
    "#  append_indicators = True appends the tokens '_start_' and '_end_' to each\n",
    "#                      document\n",
    "#  padding = 'post' means that zero padding is appended to the end of the \n",
    "#             of the document (as opposed to the default which is 'pre')\n",
    "title_pp = processor(append_indicators=True, keep_n=4500, \n",
    "                     padding_maxlen=12, padding ='post')\n",
    "\n",
    "# process the title data\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at one example of processed issue titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " typo it's > its\n",
      "after pre-processing:\n",
      " [  2 292  43  59 446   3   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_title_raw[0])\n",
    "print('after pre-processing:\\n', train_title_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize all of this to disk for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'body_pp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a4f8cbf4275f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save the preprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'body_pp.dpkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_pp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'title_pp.dpkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'body_pp' is not defined"
     ]
    }
   ],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open('title_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('train_title_vecs.npy', train_title_vecs)\n",
    "np.save('train_body_vecs.npy', train_body_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from disk into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor('title_pp.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 300\n",
    "\n",
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (ex: Issue Body)\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Intermediate GRU layer (optional)\n",
    "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles)\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Examine Model Architecture Summary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'tutorial_seq2seq'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 1200\n",
    "epochs = 7\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "seq2seq_Model.save('seq2seq_model_tutorial.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See Example Results On Holdout Set\n",
    "\n",
    "It is useful to see examples of real predictions on a holdout set to get a sense of the performance of the model.  We will also evaluate the model numerically in a following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this method displays the predictions on random rows of the holdout set\n",
    "seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluate Model: BLEU Score\n",
    "\n",
    "For machine-translation tasks such as this one, it is common to measure the accuracy of results using the [BLEU Score](https://en.wikipedia.org/wiki/BLEU).  The convenience function illustrated below uses [NLTK's corpus_bleu](https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.corpus_bleu).  The output of the below convenience function is an Average of BlEU-1, BLEU-2, BLEU-3 and BLEU-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convenience function that generates predictions on holdout set and calculates BLEU Score\n",
    "\n",
    "bleu_score = seq2seq_inf.evaluate_model(holdout_bodies=testdf.body.tolist(), \n",
    "                                        holdout_titles=testdf.issue_title.tolist(), \n",
    "                                        max_len_title=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'BLEU Score (avg of BLUE 1-4) on Holdout Set: {bleu_score * 100}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {
    "height": "263px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
